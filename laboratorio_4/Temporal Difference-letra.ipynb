{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5daa0d",
   "metadata": {},
   "source": [
    "# Métodos de Diferencias Temporales (TD)\n",
    "\n",
    "En este notebook exploraremos métodos de diferencias temporales, una familia de técnicas en Reinforcement Learning (RL) que permiten aprender a través de la experiencia sin necesidad de conocer un modelo del entorno. En particular, abordaremos dos algoritmos fundamentales:\n",
    "\n",
    "- **Sarsa (on-policy):** Un método de control en el que la política utilizada para seleccionar acciones es la misma que se evalúa y mejora.\n",
    "- **Q-Learning (off-policy):** Un método que aprende la función de valor óptima de manera independiente de la política seguida, permitiendo una mayor flexibilidad en la exploración.\n",
    "\n",
    "La implementación y el análisis se basan en el capítulo 6 del libro de Sutton y Barto, uno de los textos de referencia en el campo del aprendizaje por refuerzo.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- **Implementación del algoritmo Q-Learning:** Desarrollar y ajustar el algoritmo para estimar la función de valor óptima.\n",
    "- **Implementación del algoritmo Sarsa:** Desarrollar la versión on-policy para la estimación y mejora de la política.\n",
    "- **Comparación de desempeño:** Evaluar y comparar el desempeño de ambos algoritmos en el entorno definido, midiendo el tiempo (o cantidad de episodios) necesario para alcanzar el objetivo de manera promedio.\n",
    "- **Experimento en Cliff Walking:** Realizar el experimento clásico de Cliff Walking descrito en el libro para corroborar los conceptos y resultados presentados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8742f4",
   "metadata": {},
   "source": [
    "## Descripción del Ambiente: MountainCar\n",
    "\n",
    "El entorno [**MountainCar**](https://gymnasium.farama.org/environments/classic_control/mountain_car/) es un clásico problema de control utilizado en el ámbito del Reinforcement Learning. En este escenario, un automóvil se encuentra atrapado en un valle estrecho entre dos colinas y debe aprender a alcanzar la cima de la montaña de la derecha. Debido a las limitaciones físicas, el coche no puede simplemente acelerar de forma continua hacia la cima; en cambio, debe aprovechar la inercia y la gravedad para acumular suficiente impulso.\n",
    "\n",
    "**Espacio de Estados**\n",
    "\n",
    "El estado del entorno se define mediante dos variables continuas:\n",
    "- **Posición:** Indica la ubicación actual del automóvil sobre la pista, con un rango aproximado de \\[-0.6, 0.4\\].\n",
    "- **Velocidad:** Representa la velocidad del automóvil, y comienza siempre en 0.\n",
    "\n",
    "**Acciones Disponibles**\n",
    "\n",
    "El agente dispone de tres acciones discretas:\n",
    "- **[0] Empujar a la izquierda:** Aplica una fuerza que impulsa el coche hacia la izquierda.\n",
    "- **[1] No hacer nada:** No se aplica ninguna fuerza, permitiendo que la dinámica natural del coche influya en el movimiento.\n",
    "- **[2] Empujar a la derecha:** Aplica una fuerza que impulsa el coche hacia la derecha.\n",
    "\n",
    "**Mecánica y Dinámica del Problema**\n",
    "\n",
    "El desafío principal no radica únicamente en acelerar hacia la derecha. Debido a la gravedad y las limitaciones del motor del coche, para lograr alcanzar la cima de la montaña es necesario **aprovechar la inercia**. Esto implica que, en muchos casos, el coche debe moverse hacia la izquierda para ganar impulso y luego aprovechar esa energía para impulsarse con mayor fuerza hacia la derecha, logrando finalmente superar la pendiente.\n",
    "\n",
    "**Función de Recompensa**\n",
    "\n",
    "En cada paso del tiempo, el agente recibe una recompensa constante de **-1**. Esta penalización por cada paso incentiva al agente a encontrar la estrategia óptima que le permita alcanzar la cima de la montaña en la menor cantidad de pasos posible. Al llegar a la meta (generalmente cuando la posición del coche alcanza o supera un valor umbral, 0.5), el episodio se termina.\n",
    "\n",
    "\n",
    "![MountainCar](https://gymnasium.farama.org/_images/mountain_car.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1206ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play\n",
    "from gymnasium.wrappers import RecordVideo, TransformObservation\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import plot_rewards, plot_epsilon\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a897d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(record_video=False, record_every=1, folder=\"./videos\" ):\n",
    "    \"\"\"\n",
    "    Create the MountainCar environment with optional video recording and statistics.\n",
    "    Args:\n",
    "        record_video (bool): Whether to record video of the episodes.\n",
    "        record_every (int): Frequency of recording episodes.\n",
    "        folder (str): Folder to save the recorded videos.\n",
    "    Returns:\n",
    "        env (gym.Env): The MountainCar environment.\n",
    "        \n",
    "    See also:\n",
    "        https://gymnasium.farama.org/introduction/record_agent/\n",
    "    \"\"\"\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_video:\n",
    "        env = RecordVideo(env, video_folder=folder, name_prefix=\"MountainCar\",\n",
    "                    episode_trigger=lambda x: x % record_every == 0)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb08e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env(record_video=True, record_every=1, folder=\"./videos/random_actions\")\n",
    "\n",
    "for episode_num in range(5):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13599117",
   "metadata": {},
   "source": [
    "## Modelando el Ambiente con Q-Learning y Sarsa\n",
    "\n",
    "Tanto Q-Learning como Sarsa son algoritmos basados en **métodos tabulares**, lo que significa que requieren un espacio de estados discreto para almacenar y actualizar los valores Q asociados a cada acción en cada estado. Sin embargo, el ambiente MountainCar **presenta estados continuos**, ya que se define a partir de variables como la posición y la velocidad, lo que plantea ciertos desafíos.\n",
    "\n",
    "Debido a que estos valores pueden tomar infinitos valores posibles, resulta inviable utilizar una representación tabular directa para el aprendizaje.\n",
    "\n",
    "### Discretización de Estados\n",
    "\n",
    "¿Cómo se puede aplicar Q-Learning y Sarsa en un entorno con estados continuos? -> Discretización\n",
    "\n",
    "Para aplicar Q-Learning y Sarsa en un entorno con estados continuos, se recurre a la **discretización**. Esto consiste en dividir cada dimensión del estado en un número finito de intervalos (o \"bins\"), de modo que:\n",
    "\n",
    "- **Conversión a Estados Discretos:** Cada par de valores (posición, velocidad) se asigna a un \"bin\" o estado discreto, permitiendo indexar una tabla Q.\n",
    "- **Reducción de la Complejidad:** Al transformar el espacio continuo en un conjunto finito de estados, se simplifica el problema y se vuelve computacionalmente manejable para los métodos tabulares.\n",
    "\n",
    "### Consideraciones y Desafíos\n",
    "\n",
    "- **Precisión vs. Dimensionalidad:**  \n",
    "  Un mayor número de bins puede capturar con mayor detalle las variaciones en el estado, pero también aumenta el tamaño de la tabla Q. Esto puede requerir más datos y tiempo de entrenamiento para que el aprendizaje converja.\n",
    "\n",
    "- **Generalización:**  \n",
    "  La discretización agrupa estados cercanos en el mismo bin, lo que puede ayudar a generalizar el comportamiento del agente. Sin embargo, una discretización demasiado gruesa puede perder información importante sobre las sutilezas de la dinámica del entorno.\n",
    "\n",
    "- **Elección de Intervalos:**  \n",
    "  Es crucial seleccionar adecuadamente los intervalos para cada variable del estado. Esto suele implicar un análisis del rango y la distribución de las variables (posición y velocidad) para definir límites que capturen la dinámica del ambiente de manera efectiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6428a89",
   "metadata": {},
   "source": [
    "### Uso de `TransformObservation` para Discretizar el Ambiente\n",
    "\n",
    "Gymnasium ofrece un wrapper muy práctico llamado `TransformObservation` que permite transformar las observaciones de manera sencilla sin necesidad de definir una clase personalizada. Esto es especialmente útil cuando se quiere convertir un espacio continuo en un espacio discreto para poder utilizar métodos tabulares como Q-Learning o Sarsa.\n",
    "\n",
    "Ver [TransformObservation](https://gymnasium.farama.org/api/wrappers/observation_wrappers/#gymnasium.wrappers.TransformObservation) para más detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_STATES = 50\n",
    "\n",
    "# Definir la función de transformación para discretizar la observación.\n",
    "def discretize_obs(obs):\n",
    "    # observation_space low y high nos permiten conocer los límites de nuestro ambiente para los valores de Posicion y Velocidad.\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    env_den = (env_high - env_low) / NUMBER_STATES \n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "    \n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "    \n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df904f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_discretized(record_video=False, record_every=1, folder=\"./videos\"):\n",
    "    base_env = get_env(record_video=record_video, record_every=record_every, folder=folder)\n",
    "    # Discretizar la observación\n",
    "    new_observation_space = spaces.MultiDiscrete([NUMBER_STATES, NUMBER_STATES])\n",
    "    return TransformObservation(base_env, discretize_obs, new_observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b70ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()\n",
    "obs, info = env.reset()\n",
    "print(f\"Original observation: {obs}\")\n",
    "\n",
    "env_dis = get_env_discretized()\n",
    "obs, info = env_dis.reset()\n",
    "print(f\"Discretized observation: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467c1fe",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "Q-Learning es un algoritmo de aprendizaje por refuerzo que busca aprender la función de valor óptima $Q^*(s, a)$ para cada par de estado-acción. A través de la exploración y explotación, el agente actualiza su tabla Q utilizando el siguiente algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448ef32",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Q-Learning (Off-Policy TD Control)} \\\\[6pt]\n",
    "\\textbf{Parámetros:} & \\quad \\alpha \\in (0,1],\\ \\varepsilon > 0,\\ \\gamma \\in (0,1].\\\\[6pt]\n",
    "\\textbf{Inicializar:} & \\quad Q(s,a)\\ \\text{arbitrario para } s \\in S,\\ a \\in A(s), \\\\\n",
    "& \\quad Q(\\text{terminal},\\cdot) = 0.\\\\[6pt]\n",
    "\\textbf{Loop (por cada episodio):} & \\\\[-2pt]\n",
    "& \\quad \\text{Inicializar } S.\\\\\n",
    "& \\quad \\textbf{mientras } S \\text{ no sea terminal:}\\\\\n",
    "& \\quad\\quad A \\leftarrow \\text{acción seleccionada a partir de } S \\text{ usando política } \\varepsilon\\text{-greedy con respecto a } Q.\\\\\n",
    "& \\quad\\quad \\text{Ejecutar } A,\\ \\text{observar } R,\\ S'.\\\\\n",
    "& \\quad\\quad Q(S,A) \\leftarrow Q(S,A) \\;+\\; \\alpha \\Bigl[\\,R \\;+\\; \\gamma \\max_{a \\in A(S')}Q(S',a) \\;-\\; Q(S,A)\\Bigr].\\\\\n",
    "& \\quad\\quad S \\leftarrow S'.\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon_start=1, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "    \"\"\"\n",
    "    Q-learning algorithm for the MountainCar environment.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): The environment to train on.\n",
    "        num_episodes (int): Number of episodes to train.\n",
    "        alpha (float): Learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        epsilon_start (float): Initial exploration rate.\n",
    "        epsilon_decay (float): Decay rate for exploration.\n",
    "        epsilon_min (float): Minimum exploration rate.\n",
    "        \n",
    "    Returns:\n",
    "        Q (np.ndarray): The learned Q-table.\n",
    "        episode_rewards (list): Rewards per episode.\n",
    "        epsilons (list): Exploration rates per episode.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_EPISODES = 25_000\n",
    "\n",
    "env_dis = get_env_discretized(record_video=True, record_every=1_000, folder=\"./videos/q_learning_training\")\n",
    "Q_ql, rewards_ql, epsilons_ql = q_learning(env_dis, num_episodes=NUMBER_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db527d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards_ql)\n",
    "plot_epsilon(epsilons_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dis = get_env_discretized(record_video=True, record_every=1, folder=\"./videos/q_learning_test\")\n",
    "\n",
    "for episode_num in range(5):\n",
    "    (pos, vel), info = env_dis.reset()\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = np.argmax(Q_ql[pos, vel])  # Exploit\n",
    "        (pos, vel), reward, terminated, truncated, info = env_dis.step(action)\n",
    "        episode_over = terminated or truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea57d1",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "Sarsa es otro algoritmo de aprendizaje por refuerzo que, a diferencia de Q-Learning, es un método on-policy. Esto significa que el agente actualiza su tabla Q utilizando la acción que realmente toma en el entorno, lo que lo hace más sensible a la política actual. El algoritmo se basa en el siguiente algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca9d0a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Sarsa (On-Policy TD Control)} \\\\[6pt]\n",
    "\\textbf{Parámetros:} & \\quad \\alpha \\in (0,1],\\ \\varepsilon > 0,\\ \\gamma \\in (0,1].\\\\[6pt]\n",
    "\\textbf{Inicializar:} & \\quad Q(s,a)\\ \\text{arbitrario para } s \\in S,\\ a \\in A(s), \\\\\n",
    "& \\quad Q(\\text{terminal},\\cdot) = 0.\\\\[6pt]\n",
    "\\textbf{Loop (por cada episodio):} & \\\\[-2pt]\n",
    "& \\quad \\text{Inicializar } S \\text{ y elegir } A \\text{ usando política } \\varepsilon\\text{-greedy con respecto a } Q.\\\\\n",
    "& \\quad \\textbf{mientras } S \\text{ no sea terminal:}\\\\\n",
    "& \\quad\\quad \\text{Ejecutar } A,\\ \\text{observar } R,\\ S'.\\\\\n",
    "& \\quad\\quad \\text{Elegir } A' \\text{ a partir de } S' \\text{ usando política } \\varepsilon\\text{-greedy con respecto a } Q.\\\\\n",
    "& \\quad\\quad Q(S,A) \\leftarrow Q(S,A) \\;+\\; \\alpha \\Bigl[\\,R \\;+\\; \\gamma\\,Q(S',A') \\;-\\; Q(S,A)\\Bigr].\\\\\n",
    "& \\quad\\quad S \\leftarrow S',\\quad A \\leftarrow A'.\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q, pos, vel, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action using epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        Q (np.ndarray): The Q-table.\n",
    "        state (tuple): The current state.\n",
    "        epsilon (float): The exploration rate.\n",
    "        \n",
    "    Returns:\n",
    "        action (int): The selected action.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[pos][vel])  # Exploit\n",
    "\n",
    "def sarsa(env, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon_start=1, epsilon_decay=0.9995, epsilon_min=0.05):\n",
    "    \"\"\"\n",
    "    SARSA algorithm for the MountainCar environment.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): The environment to train on.\n",
    "        num_episodes (int): Number of episodes to train.\n",
    "        alpha (float): Learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        epsilon_start (float): Initial exploration rate.\n",
    "        epsilon_decay (float): Decay rate for exploration.\n",
    "        epsilon_min (float): Minimum exploration rate.\n",
    "        \n",
    "    Returns:\n",
    "        Q (np.ndarray): The learned Q-table.\n",
    "        episode_rewards (list): Rewards per episode.\n",
    "        epsilons (list): Exploration rates per episode.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dis = get_env_discretized(record_video=True, record_every=1_000, folder=\"./videos/sarsa_training\")\n",
    "Q_sarsa, rewards_sarsa, epsilons_sarsa = sarsa(env_dis, num_episodes=NUMBER_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards_sarsa)\n",
    "plot_epsilon(epsilons_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e97689",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dis = get_env_discretized(record_video=True, record_every=1, folder=\"./videos/sarsa_test\")\n",
    "\n",
    "for episode_num in range(5):\n",
    "    (pos, vel), info = env_dis.reset()\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = np.argmax(Q_sarsa[pos, vel])  # Exploit\n",
    "        (pos, vel), reward, terminated, truncated, info = env_dis.step(action)\n",
    "        episode_over = terminated or truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99fd6f",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "1. Implementar tanto el algoritmo de Q-Learning como el de Sarsa para estimar la función Q de la política definida.\n",
    "2. Implementar el experimento de Cliff Walking, tal como se describe en el libro, para comparar ambos algoritmos. Analizar y discutir los resultados, en particular, identificar cuál de los dos algoritmos tiende a \"caminar por el borde\" (tomando más riesgos) y cuál es más cauteloso. Utilizar el entorno [CliffWalking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
