{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tarea 1 - K Armed Bandits\n",
    "\n",
    "## Objetivos\n",
    "- Demostrar entendimiento sobre K-Armed Bandits\n",
    "- Implementar varios algoritmos vistos en el curso teórico \n",
    "- comparar su performance en un mismo problema (ambiente).\n",
    "\n",
    "## A entregar\n",
    "- Implementación de agente arbitrario.\n",
    "- Implementación de agente epsilon-greedy.\n",
    "- Implementación de agente epsilon-greedy optimista.\n",
    "- Implementación de agente upper confidence bound.\n",
    "- Grafique la evolución de las recompensas (reward) a lo largo del tiempo para cada algoritmo, promediado sobre 2000 ejecuciones.\n",
    "- Escriba un reporte sobre sus implementaciones comparando los distintos agentes implementados.\n",
    "- [Extra] Implementación de epsilon-greedy con decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del ambiente a usar\n",
    "\n",
    "Vamos a usar KBanditsEnv, un ambiente implementado con [Gymnasium](https://gymnasium.farama.org/index.html) pero orientada a [Armed Bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit). \n",
    "\n",
    "Recordando el teórico, sabemos que un K Armed Bandit representa una maquina tragamonedas con multiples brazos (K) y donde nuestra recompensa depende de cual brazo tiremos.\n",
    "\n",
    "En este caso particular, `k_bandits_env/KBanditsGaussian-v0` es el ambiente a utilizar (página 28 del libro de Sutton y Barto), cuenta con 10 brazos (K=10) y por cada interaccion con el ambiente (`step`) nustra recompensa (reward) depende de que brazo elegimos entre los 10 posibles. En este ambiente, todos los brazos tienen una recompensa (que puede ser negativa en algunos casos) y nuestro objetivo es diseñar agentes que maximicen la suma total de recompensas a lo largo de 1000 interacciones con el ambiente (`1000 steps`).\n",
    "\n",
    "![KBandits](https://miro.medium.com/v2/resize:fit:720/format:webp/1*iguhq3SaQd730c-TYI8QZA.png)\n",
    "\n",
    "El valor verdadero ($q_*(a)$) de cada brazo es muestreado de una distribución normal con media 0 y varianza 1. La recompensa de cada brazo es muestreada de una distribución normal con media $q_*(a)$ y varianza 1.\n",
    "\n",
    "\n",
    "> Nota: El agente no tiene acceso a los valores verdaderos de los brazos, solo puede interactuar con el ambiente, es decir, seleccionar un brazo y recibir una recompensa. Nosotros usaremos los valores verdaderos para comparar la performance de los distintos agentes.\n",
    "\n",
    "Algunas preguntas:\n",
    "- ¿Es un ambiente estacionario o no estacionario?\n",
    "- ¿Qué se espera de un agente que seleccione brazos de manera aleatoria en terminos de recompensa y % de veces que selecciona el mejor brazo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del ambiente e imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from Agent import Agent\n",
    "from Utils import get_env, plot_agents_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas constantes que vamos a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 1000\n",
    "NUM_EPISODES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un entorno de ejemplo, con print_true_values podemos ver los valores verdaderos de las acciones\n",
    "# PD: en teoria no deberiamos tener acceso a estos valores pero nos sirve para comparar\n",
    "sample_env = get_env(print_true_values=True) \n",
    "print(sample_env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Agentes\n",
    "\n",
    "En esta sección, exploraremos cómo crear agentes concretos heredando de la clase abstracta `Agent`. Esta clase base define la estructura y el comportamiento general que cualquier agente debe seguir al interactuar con un entorno, proporcionando un marco consistente para diversas estrategias de toma de decisiones.\n",
    "\n",
    "\n",
    "**Pasos para Implementar un Agente Concreto**\n",
    "\n",
    "1. **Definir el Constructor (`__init__`)**: Inicializa los atributos específicos del agente, incluyendo su nombre y cualquier parámetro adicional necesario para su estrategia particular.\n",
    "\n",
    "2. **Implementar `reset_internal_state`**: Establece o reinicia las variables internas que el agente utiliza para rastrear información durante su interacción con el entorno.\n",
    "\n",
    "3. **Implementar `select_action`**: Define la lógica que el agente emplea para seleccionar una acción basada en la observación actual, la información proporcionada por el entorno y el estado interno del agente.\n",
    "\n",
    "4. **Implementar `update_internal_state`**: Actualiza el estado interno del agente en función de la nueva información obtenida tras ejecutar una acción, como la nueva observación y la recompensa recibida.\n",
    "\n",
    "5. **Implementar `get_extra_info`**: Proporciona información adicional sobre el estado o desempeño del agente, útil para análisis o depuración.\n",
    "\n",
    "\n",
    "Preguntas que pueden ayudar a guiar la implementación de los agentes:\n",
    "- ¿Qué información necesita el agente para tomar decisiones?\n",
    "- ¿Cómo se actualiza el estado interno del agente después de cada interacción con el entorno?\n",
    "- ¿Qué estrategias de selección de acciones basados en el conocimiento previo y la información actual se pueden utilizar?\n",
    "\n",
    "**Evaluación del Desempeño de los Agentes**\n",
    "\n",
    "Para analizar cómo se comportan nuestros agentes en el entorno, utilizaremos dos métricas clave:\n",
    "\n",
    "1. **Recompensa Promedio por Paso:** Esta métrica nos muestra la recompensa media que un agente obtiene en cada paso a lo largo de múltiples episodios. Un valor más alto indica que el agente está tomando decisiones que le reportan mayores beneficios en promedio.\n",
    "\n",
    "2. **Porcentaje de Selección de la Acción Óptima por Paso:** Esta métrica refleja la frecuencia con la que un agente selecciona la mejor acción disponible en cada paso. Un porcentaje elevado sugiere que el agente identifica y elige consistentemente la acción más beneficiosa.\n",
    "\n",
    "Para obtener estas métricas, ejecutaremos cada agente durante un número determinado de episodios (`NUM_EPISODES`) y pasos por episodio (`NUM_STEPS`). Durante estas ejecuciones, registraremos las recompensas obtenidas y las acciones seleccionadas. Posteriormente, calcularemos el promedio de las recompensas y el porcentaje de veces que se seleccionó la acción óptima en cada paso.\n",
    "\n",
    "Estas evaluaciones nos permitirán comparar el rendimiento de diferentes agentes y entender mejor sus comportamientos en el entorno propuesto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente arbitrario (random)\n",
    "\n",
    "Este agente selecciona una acción de manera aleatoria en cada paso, sin considerar ninguna información adicional. Es un agente de referencia simple que nos permitirá comparar el rendimiento de otros agentes más sofisticados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)        \n",
    "\n",
    "    def reset_internal_state(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self, obs, info):\n",
    "        pass\n",
    "   \n",
    "    def update_internal_state(self, observation, action, reward, info):\n",
    "        pass\n",
    "    \n",
    "    def get_extra_info(self):\n",
    "        pass\n",
    "\n",
    "# Example run\n",
    "random_agent = RandomAgent(\"RandomAgent\")\n",
    "\n",
    "logs, info = random_agent.play(n_steps = NUM_STEPS, environment = sample_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este modo, podemos tener una estimación de la recompensa ($Q(a)$) de cada brazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_action = list(map(lambda x: np.array(x), logs['actions_rewards'].values())) # lista de recompensas por acción\n",
    "labels = list(logs['actions_rewards'].keys()) # lista de etiquetas de las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(rewards_per_action, tick_labels=labels)\n",
    "plt.title('Estimated Rewards per Action')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Coincide con el valor verdadero de cada brazo ($q_*(a)$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agents_performance([RandomAgent(\"RandomAgent\")], num_steps=NUM_STEPS, num_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente epsilon-greedy\n",
    "\n",
    "El agentes epsilon-greedy es una estrategia simple que selecciona la mejor acción con probabilidad $1-\\varepsilon$ y una acción aleatoria con probabilidad $\\varepsilon$.\n",
    "\n",
    "**Initialize, for** $a = 1$ to $k$:\n",
    "\n",
    "$$\n",
    "Q(a) \\gets 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "N(a) \\gets 0\n",
    "$$\n",
    "\n",
    "**Loop forever:**\n",
    "\n",
    "$$\n",
    "A \\gets \n",
    "\\begin{cases} \n",
    "\\arg\\max_a Q(a) & \\text{with probability } 1 - \\varepsilon \\quad \\text{(breaking ties randomly)} \\\\\n",
    "\\text{a random action} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R \\gets \\text{bandit}(A)\n",
    "$$\n",
    "\n",
    "$$\n",
    "N(A) \\gets N(A) + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(A) \\gets Q(A) + \\frac{1}{N(A)} \\left[ R - Q(A) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, name, epsilon):\n",
    "        super().__init__(name)        \n",
    "\n",
    "    def reset_internal_state(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self, obs, info):\n",
    "        pass\n",
    "   \n",
    "    def update_internal_state(self, observation, action, reward, info):\n",
    "        pass\n",
    "    \n",
    "    def get_extra_info(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "epsilon_greedy_agent = EpsilonGreedyAgent(\"EpsilonGreedyAgent\", 0.1)\n",
    "\n",
    "logs, info = epsilon_greedy_agent.play(n_steps = 1000, environment =sample_env)\n",
    "\n",
    "print(info['n_a'])\n",
    "print(info['q_a'])\n",
    "print (f\"Accumulated reward: {sum(logs['rewards']):.2f}\")\n",
    "print (f\"Mean reward: {logs['rewards'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agents_performance([\n",
    "    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=0.1\", 0.1),\n",
    "    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=0.01\", 0.01),\n",
    "    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=0\", 0)\n",
    "    ], num_steps=NUM_STEPS, num_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sugerencia:\n",
    "> - Probar con otros valores de $\\epsilon$ y comparar el rendimiento de los agentes. ¿Cómo afecta el valor de $\\epsilon$ al rendimiento del agente?\n",
    "> - ¿Qué ocurre si incrementamos la cantidad de pasos de entrenamiento? ¿Cómo afecta esto al rendimiento de los agentes?\n",
    ">\n",
    ">```python\n",
    ">plot_agents_performance([\n",
    ">    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=a\", a),\n",
    ">    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=b\", b),\n",
    ">    ...\n",
    ">    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=z\", z)\n",
    ">    ], num_steps=NUM_STEPS, num_episodes=NUM_EPISODES)\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente epsilon-greedy optimista\n",
    "\n",
    "El agente epsilon-greedy optimista es una variante del agente epsilon-greedy que inicializa los valores de recompensa estimados ($Q(a)$) con un valor optimista, en lugar de cero. Esto fomenta la exploración de acciones menos conocidas al principio, ya que el agente cree que todas las acciones tienen un valor más alto del que realmente tienen.\n",
    "\n",
    "Preguntas:\n",
    "- ¿Qué ocurre si ponemos un valor optimista muy alto?\n",
    "- ¿Qué ocurre si ponemos un valor optimista muy bajo?\n",
    "\n",
    "**Initialize, for** $a = 1$ to $k$:\n",
    "\n",
    "$$\n",
    "Q(a) \\gets q_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "N(a) \\gets 0\n",
    "$$\n",
    "\n",
    "**Loop forever:**\n",
    "\n",
    "$$\n",
    "A \\gets \n",
    "\\begin{cases} \n",
    "\\arg\\max_a Q(a) & \\text{with probability } 1 - \\varepsilon \\quad \\text{(breaking ties randomly)} \\\\\n",
    "\\text{a random action} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R \\gets \\text{bandit}(A)\n",
    "$$\n",
    "\n",
    "$$\n",
    "N(A) \\gets N(A) + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(A) \\gets Q(A) + \\alpha (R - Q(A))\n",
    "$$\n",
    "\n",
    "> Nota: para esta implementación, usamos $alpha$ en lugar de $\\frac{1}{N(A)}$ para actualizar el valor de $Q(A)$. Ver capítulo 2.5 del libro de Sutton y Barto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptimisticEpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, name, epsilon, q_1, alpha = 0.1):\n",
    "        super().__init__(name)\n",
    "        pass       \n",
    "\n",
    "    def reset_internal_state(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self, obs, info):\n",
    "        pass\n",
    "   \n",
    "    def update_internal_state(self, observation, action, reward, info):\n",
    "        pass\n",
    "    \n",
    "    def get_extra_info(self):\n",
    "        pass\n",
    "\n",
    "# Example run\n",
    "optimistic_epsilon_greedy_agent = OptimisticEpsilonGreedyAgent(\"OptimisticEpsilonGreedyAgent\", epsilon = 0.1, q_1 = 0)\n",
    "\n",
    "logs, info = optimistic_epsilon_greedy_agent.play(n_steps = 1000, environment = sample_env)\n",
    "print(info['n_a'])\n",
    "print(info['q_a'])\n",
    "print (f\"Accumulated reward: {sum(logs['rewards']):.2f}\")\n",
    "print (f\"Mean reward: {logs['rewards'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agents_performance([\n",
    "    OptimisticEpsilonGreedyAgent(\"OptimisticEpsilonGreedyAgent e=0.1 Q1=0\", epsilon = 0.1, q_1 = 0),\n",
    "    OptimisticEpsilonGreedyAgent(\"OptimisticEpsilonGreedyAgent e=0   Q1=5\", epsilon = 0,   q_1 = 5),\n",
    "    ], num_steps=NUM_STEPS, num_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente Upper Confidence Bound (UCB)\n",
    "\n",
    "El agente UCB es una estrategia de selección de acciones que equilibra de forma inteligente la exploración y la explotación. En lugar de elegir acciones de forma aleatoria con cierta probabilidad (como en epsilon-greedy), el agente UCB asigna a cada acción una \"bonificación de exploración\" que disminuye conforme la acción es seleccionada más veces. Esta bonificación se combina con la estimación actual del valor $Q(a)$ para formar un criterio de selección.\n",
    "\n",
    "Para cada acción $a$, se calcula:\n",
    "\n",
    "$$\n",
    "UCB(a) = Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $Q(a)$ es la estimación del valor esperado de la acción $a$.\n",
    "- $N(a)$ es el número de veces que se ha seleccionado la acción $a$.\n",
    "- $t$ es el número total de pasos (o iteraciones) realizados hasta el momento.\n",
    "- $c$ es un parámetro que determina el grado de exploración: un valor mayor favorece la exploración de acciones menos probadas.\n",
    "\n",
    "**Inicialización:**\n",
    "\n",
    "Para cada acción $a = 1, \\dots, k$:\n",
    "\n",
    "$$\n",
    "Q(a) \\gets 0\n",
    "$$\n",
    "$$\n",
    "N(a) \\gets 0\n",
    "$$\n",
    "$$\n",
    "t \\gets 0\n",
    "$$\n",
    "\n",
    "**Algoritmo:**\n",
    "\n",
    "**Loop forever:**\n",
    "\n",
    "   $$\n",
    "   t \\gets t + 1\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   A \\gets \\arg\\max_a \\left\\{ Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right\\}\n",
    "   $$\n",
    "   $$\n",
    "   R \\gets \\text{bandit}(A)\n",
    "   $$\n",
    "   $$\n",
    "   N(A) \\gets N(A) + 1\n",
    "   $$\n",
    "   $$\n",
    "   Q(A) \\gets Q(A) + \\frac{1}{N(A)} \\left( R - Q(A) \\right)\n",
    "   $$\n",
    "   \n",
    ">Notas: \n",
    ">   - Para aquellas acciones que aún no han sido seleccionadas ($N(a) = 0$), se suele asignar un valor infinito al término de exploración para garantizar que cada acción se pruebe al menos una vez.\n",
    ">   - Se puede usar una tasa de aprendizaje $\\alpha$ para actualizar $Q(A)$ en lugar de $\\frac{1}{N(A)}$.\n",
    "\n",
    "### Preguntas:\n",
    "\n",
    "- **¿Qué sucede si el parámetro $c$ es muy alto?**\n",
    "- **¿Qué sucede si el parámetro $c$ es muy bajo?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent(Agent):\n",
    "    def __init__(self, name, C):\n",
    "        super().__init__(name) \n",
    "\n",
    "    def reset_internal_state(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self, obs, info):\n",
    "        pass\n",
    "        \n",
    "   \n",
    "    def update_internal_state(self, observation, action, reward, info):\n",
    "        pass\n",
    "    \n",
    "    def get_extra_info(self):\n",
    "        pass\n",
    "\n",
    "# Example run\n",
    "UCB_agent = UCBAgent(\"UCBAgent\", 1)\n",
    "\n",
    "logs, info = UCB_agent.play(n_steps = 1000, environment =sample_env)\n",
    "print(info['n_a'])\n",
    "print(info['q_a'])\n",
    "print (f\"Accumulated reward: {sum(logs['rewards']):.2f}\")\n",
    "print (f\"Mean reward: {logs['rewards'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agents_performance([\n",
    "    UCBAgent(\"UCB Agent C=2\", 2),\n",
    "    EpsilonGreedyAgent(\"EpsilonGreedyAgent e=0.1\", 0.1),\n",
    "    ], num_steps=NUM_STEPS, num_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "1. Implementar los agentes mencionados.\n",
    "2. Comentar los resultados obtenidos en las gráficas.\n",
    "3. Seleccionar de cada agente el/los mejores hiperparámetros.\n",
    "4. Tomar el mejor candidato de cada agente y compararlos en un mismo gráfico.\n",
    "5. Explique porque en algunos agentes sus graficas tienen picos (epsilon-greedy optimista y UCB).\n",
    "6. [Extra] Implementar un agente [epsilon-greedy con decay](https://arxiv.org/pdf/1910.13701)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
