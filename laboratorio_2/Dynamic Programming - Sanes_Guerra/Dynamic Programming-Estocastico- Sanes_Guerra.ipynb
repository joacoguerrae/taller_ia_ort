{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboratorio 2 - Entorno estocástico -  Ramiro Sanes (368397) y Joaquín Guerra (307854)\n",
    "\n",
    "# Evaluación en un entorno estocástico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import numpy as np\n",
    "import GridWorldEnv\n",
    "\n",
    "from Utils import print_state_values, print_state_action_values, print_policy_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "HUMAN_RENDER = False # False si queremos guardar el episodio en un video, True si queremos verlo en tiempo real (no disponible en Google Colab u otros entornos sin interfaz gráfica)\n",
    "\n",
    "RECORD_VIDEO = False # True si queremos guardar el episodio en un video, quizas se necesita: https://anaconda.org/conda-forge/gymnasium-other\n",
    "RECORD_EVERY = 1 # Cada cuántos episodios guardamos un video\n",
    "\n",
    "GRID_SIZE = 4 # Tamaño del grid\n",
    "GAMMA = 1 # Factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Wrapper\n",
    "\n",
    "En Gymnasium, los [wrappers](https://gymnasium.farama.org/api/wrappers/) son herramientas que permiten modificar el comportamiento de los entornos sin alterar su código base. Estos *wrappers* se utilizan para extender funcionalidades, como la normalización de observaciones, limitación de acciones o registro de estadísticas y videos durante el entrenamiento de agentes.\n",
    "\n",
    "Para este ejercicio, se utilizarán dos *wrappers* de Gymnasium:\n",
    "\n",
    "**`RecordVideo`**: Este *wrapper* permite grabar videos de las ejecuciones del agente en el entorno. Es especialmente útil para visualizar el comportamiento del agente y evaluar su desempeño. Se puede configurar para grabar todos los episodios o solo episodios específicos, según una función de activación definida por el usuario.\n",
    "\n",
    "Documentación oficial de Gymnasium sobre [grabación de agentes](https://gymnasium.farama.org/introduction/record_agent/) y la [API de wrappers](https://gymnasium.farama.org/api/wrappers/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment(human_render=HUMAN_RENDER, record_video=RECORD_VIDEO, record_every=RECORD_EVERY, grid_size=GRID_SIZE):\n",
    "    assert not (human_render and record_video), \"No se puede renderizar en tiempo real y guardar un video a la vez\"\n",
    "    \n",
    "    render_mode = \"human\" if human_render else \"rgb_array\"\n",
    "    \n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"gymnasium_env/GridWorld_stochastic-v0\", render_mode=render_mode, size=grid_size)\n",
    "\n",
    "    if record_video:\n",
    "        env = RecordVideo(env, video_folder=\"./videos\", name_prefix=\"gridworld\",\n",
    "                    episode_trigger=lambda x: x % record_every == 0)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el entorno y sus wrappers con un agente aleatorio para visualizar el comportamiento del entorno y la grabación de videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export IMAGEIO_FFMPEG_EXE\n",
    "#import os\n",
    "#os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "# We play 10 episodes, each one with a random policy\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # random policy\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de la Política $\\pi$\n",
    "\n",
    "Para implementar los algoritmos de programación dinámica, necesitamos definir cómo representaremos la política $\\pi(s, a)$. \n",
    "\n",
    "### Estructura de la Política\n",
    "La política se almacenará en un **diccionario** de la forma:\n",
    "\n",
    "```python\n",
    "pi = {\n",
    "    ((x, y), a): probabilidad\n",
    "}\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- $(x, y)$ representa el estado en la grilla.\n",
    "- $a$ representa una de las cuatro acciones posibles.\n",
    "- `probabilidad` es la probabilidad de tomar la acción $a$ en el estado $(x, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función auxiliar para saber si estamos en un estado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(state):\n",
    "    return state in [(0, 0), (GRID_SIZE - 1, GRID_SIZE - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Uniforme\n",
    "Una política uniforme en la que todas las acciones tienen la misma probabilidad ($ 25\\% $) se define como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 1), 0): 0.25, ((0, 1), 1): 0.25, ((0, 1), 2): 0.25, ((0, 1), 3): 0.25, ((0, 2), 0): 0.25, ((0, 2), 1): 0.25, ((0, 2), 2): 0.25, ((0, 2), 3): 0.25, ((0, 3), 0): 0.25, ((0, 3), 1): 0.25, ((0, 3), 2): 0.25, ((0, 3), 3): 0.25, ((1, 0), 0): 0.25, ((1, 0), 1): 0.25, ((1, 0), 2): 0.25, ((1, 0), 3): 0.25, ((1, 1), 0): 0.25, ((1, 1), 1): 0.25, ((1, 1), 2): 0.25, ((1, 1), 3): 0.25, ((1, 2), 0): 0.25, ((1, 2), 1): 0.25, ((1, 2), 2): 0.25, ((1, 2), 3): 0.25, ((1, 3), 0): 0.25, ((1, 3), 1): 0.25, ((1, 3), 2): 0.25, ((1, 3), 3): 0.25, ((2, 0), 0): 0.25, ((2, 0), 1): 0.25, ((2, 0), 2): 0.25, ((2, 0), 3): 0.25, ((2, 1), 0): 0.25, ((2, 1), 1): 0.25, ((2, 1), 2): 0.25, ((2, 1), 3): 0.25, ((2, 2), 0): 0.25, ((2, 2), 1): 0.25, ((2, 2), 2): 0.25, ((2, 2), 3): 0.25, ((2, 3), 0): 0.25, ((2, 3), 1): 0.25, ((2, 3), 2): 0.25, ((2, 3), 3): 0.25, ((3, 0), 0): 0.25, ((3, 0), 1): 0.25, ((3, 0), 2): 0.25, ((3, 0), 3): 0.25, ((3, 1), 0): 0.25, ((3, 1), 1): 0.25, ((3, 1), 2): 0.25, ((3, 1), 3): 0.25, ((3, 2), 0): 0.25, ((3, 2), 1): 0.25, ((3, 2), 2): 0.25, ((3, 2), 3): 0.25}\n"
     ]
    }
   ],
   "source": [
    "pi_rand = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        for a in range(4):  # Cuatro acciones posibles\n",
    "            if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "                continue\n",
    "            pi_rand[((x, y), a)] = 1 / 4  # Probabilidad uniforme\n",
    "\n",
    "print(pi_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Determinista (derecha y abajo)\n",
    "Si queremos que el agente siempre elija moverse a la derecha (acción `0`) a excepción de la ultima columna que se mueve hacia abajo (accion `3`), podemos definir la política de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 1), 0): 1.0, ((0, 2), 0): 1.0, ((0, 3), 0): 1.0, ((1, 0), 0): 1.0, ((1, 1), 0): 1.0, ((1, 2), 0): 1.0, ((1, 3), 0): 1.0, ((2, 0), 0): 1.0, ((2, 1), 0): 1.0, ((2, 2), 0): 1.0, ((2, 3), 0): 1.0, ((3, 0), 3): 1, ((3, 1), 3): 1, ((3, 2), 3): 1}\n"
     ]
    }
   ],
   "source": [
    "pi_der_aba = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "            continue\n",
    "        if x == GRID_SIZE - 1:  # Si estamos en la última columna, solo podemos movernos hacia abajo\n",
    "            pi_der_aba[((x, y), 3)] = 1\n",
    "        else:  # En otro caso, podemos movernos hacia la derecha\n",
    "            pi_der_aba[((x, y), 0)] = 1.0\n",
    "print(pi_der_aba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Determinista (siempre abajo)\n",
    "Si queremos que el agente siempre elija moverse hacia abajo (acción `3`), podemos definir la política de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_abajo = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "                continue\n",
    "        pi_abajo[((x, y), 3)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de una acción dado un estado y una política\n",
    "Para elegir una acción dado un estado y una política, necesitamos muestrear una acción de acuerdo a la distribución de probabilidad definida en la política. Para esto, podemos utilizar la función `np.random.choice` de NumPy, que nos permite muestrear un elemento de un conjunto dado con una probabilidad dada.\n",
    "\n",
    "Más información sobre `np.random.choice` en la [documentación oficial de NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_action(state, policy):\n",
    "    \"\"\"\n",
    "    Dado un estado y una política, devuelve la acción a tomar.\n",
    "\n",
    "    Parámetros:\n",
    "    - state: Tupla (x, y) representando el estado actual.\n",
    "    - policy: Diccionario {((x, y), a): probabilidad} con la política.\n",
    "\n",
    "    Retorna:\n",
    "    - La acción seleccionada según la política.\n",
    "    \n",
    "    Nota: Si la política es estocástica, se selecciona una acción al azar según las probabilidades.\n",
    "    \"\"\"\n",
    "    actions = range(4)\n",
    "\n",
    "    prob = [policy.get((state, a), 0) for a in actions]\n",
    "\n",
    "    return np.random.choice(actions, p=prob)\n",
    "\n",
    "select_action((2,2), pi_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinámica del Ambiente ($ p $) \n",
    "\n",
    "**La dinámica del ambiente $ p(s' | s, a) $**, que define las transiciones entre estados al tomar una acción. Esta información nos permite conocer, para cada estado y acción, cuál será la probabilidad de llegar al próximo estado y la recompensa recibida.\n",
    "\n",
    "> Cuando creamos un ambiente con `gym.make()`, Gymnasium aplica un envoltorio (`OrderEnforcing`) que restringe el acceso directo a ciertos atributos internos. Para acceder a `p`, necesitamos usar `env.unwrapped`, que nos da acceso al objeto subyacente sin restricciones.\n",
    "\n",
    "Ejemplo de acceso a la política desde el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 0), 0): [(1.0, (0, 0), 0.0)], ((0, 0), 1): [(1.0, (0, 0), 0.0)], ((0, 0), 2): [(1.0, (0, 0), 0.0)], ((0, 0), 3): [(1.0, (0, 0), 0.0)], ((0, 1), 0): [(0.8, (1, 1), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)], ((0, 1), 1): [(0.8, (0, 0), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)], ((0, 1), 2): [(0.8, (0, 1), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)], ((0, 1), 3): [(0.8, (0, 2), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)], ((0, 2), 0): [(0.8, (1, 2), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)], ((0, 2), 1): [(0.8, (0, 1), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)], ((0, 2), 2): [(0.8, (0, 2), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)], ((0, 2), 3): [(0.8, (0, 3), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)], ((0, 3), 0): [(0.8, (1, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)], ((0, 3), 1): [(0.8, (0, 2), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)], ((0, 3), 2): [(0.8, (0, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)], ((0, 3), 3): [(0.8, (0, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)], ((1, 0), 0): [(0.8, (2, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)], ((1, 0), 1): [(0.8, (1, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)], ((1, 0), 2): [(0.8, (0, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)], ((1, 0), 3): [(0.8, (1, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)], ((1, 1), 0): [(0.8, (2, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)], ((1, 1), 1): [(0.8, (1, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)], ((1, 1), 2): [(0.8, (0, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)], ((1, 1), 3): [(0.8, (1, 2), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)], ((1, 2), 0): [(0.8, (2, 2), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)], ((1, 2), 1): [(0.8, (1, 1), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)], ((1, 2), 2): [(0.8, (0, 2), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)], ((1, 2), 3): [(0.8, (1, 3), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)], ((1, 3), 0): [(0.8, (2, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)], ((1, 3), 1): [(0.8, (1, 2), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)], ((1, 3), 2): [(0.8, (0, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)], ((1, 3), 3): [(0.8, (1, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)], ((2, 0), 0): [(0.8, (3, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)], ((2, 0), 1): [(0.8, (2, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)], ((2, 0), 2): [(0.8, (1, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)], ((2, 0), 3): [(0.8, (2, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)], ((2, 1), 0): [(0.8, (3, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)], ((2, 1), 1): [(0.8, (2, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)], ((2, 1), 2): [(0.8, (1, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)], ((2, 1), 3): [(0.8, (2, 2), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)], ((2, 2), 0): [(0.8, (3, 2), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)], ((2, 2), 1): [(0.8, (2, 1), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)], ((2, 2), 2): [(0.8, (1, 2), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)], ((2, 2), 3): [(0.8, (2, 3), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)], ((2, 3), 0): [(0.8, (3, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)], ((2, 3), 1): [(0.8, (2, 2), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)], ((2, 3), 2): [(0.8, (1, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)], ((2, 3), 3): [(0.8, (2, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)], ((3, 0), 0): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)], ((3, 0), 1): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)], ((3, 0), 2): [(0.8, (2, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)], ((3, 0), 3): [(0.8, (3, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)], ((3, 1), 0): [(0.8, (3, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)], ((3, 1), 1): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)], ((3, 1), 2): [(0.8, (2, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)], ((3, 1), 3): [(0.8, (3, 2), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)], ((3, 2), 0): [(0.8, (3, 2), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)], ((3, 2), 1): [(0.8, (3, 1), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)], ((3, 2), 2): [(0.8, (2, 2), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)], ((3, 2), 3): [(0.8, (3, 3), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)], ((3, 3), 0): [(1.0, (3, 3), 0.0)], ((3, 3), 1): [(1.0, (3, 3), 0.0)], ((3, 3), 2): [(1.0, (3, 3), 0.0)], ((3, 3), 3): [(1.0, (3, 3), 0.0)]}\n"
     ]
    }
   ],
   "source": [
    "p = env.unwrapped.p\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario tiene la siguiente forma:\n",
    "\n",
    "```python\n",
    "self.p = {\n",
    "    ((x, y), a): [(probabilidad, (nuevo_x, nuevo_y), recompensa)]\n",
    "}\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `((x, y), a)`: Clave que representa el **estado actual** `(x, y)` y la **acción** `a` tomada.\n",
    "- **Valor asociado**: Una lista con **una o más transiciones posibles** en el formato:\n",
    "  - `probabilidad`: La probabilidad de que ocurra la transición (en este caso, siempre `1.0`, porque el ambiente es determinista).\n",
    "  - `(nuevo_x, nuevo_y)`: El nuevo estado después de tomar la acción.\n",
    "  - `recompensa`: El valor de recompensa por realizar la acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso al tener una dinámica del ambiente estocástica, observamos que en algunos estados, tomando alguna acción se puede llegar a 3 distintos pares (estado siguiente, recompensa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(i) for i in p.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "La **Iteración de Política** (*Policy Iteration*) es un método fundamental en **Programación Dinámica** para encontrar la política óptima $\\pi^*$ en un proceso de decisión de Markov (*MDP*) finito. Se basa en dos pasos clave que se repiten iterativamente hasta la convergencia:\n",
    "\n",
    "1. **Evaluación de Política** (*Policy Evaluation*): Se calcula el valor de la política actual $\\pi$, es decir, se obtiene la función de valor $V_\\pi(s)$ resolviendo la ecuación de Bellman para todos los estados:\n",
    "   $$\n",
    "   V_\\pi(s) = \\sum_{a} \\pi(a | s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V_\\pi(s') \\right]\n",
    "   $$\n",
    "   donde $p(s', r | s, a)$ representa la dinámica del ambiente y $\\gamma$ es el factor de descuento.\n",
    "\n",
    "2. **Mejora de Política** (*Policy Improvement*): Se actualiza la política eligiendo en cada estado la acción que maximiza el valor esperado según la función de acción-valor $Q_\\pi(s, a)$:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_{a} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V_\\pi(s') \\right]\n",
    "   $$\n",
    "   Si la nueva política $\\pi'$ es diferente de la anterior, se repite el proceso con la nueva política. Si no cambia, hemos encontrado la política óptima $\\pi^*$.\n",
    "\n",
    "Proceso Iterativo:\n",
    "1. Inicializar una política arbitraria $\\pi$.\n",
    "2. Aplicar **Policy Evaluation** para calcular $V_\\pi$.\n",
    "3. Aplicar **Policy Improvement** para obtener una nueva política $\\pi'$.\n",
    "4. Si $\\pi' = \\pi$, detenerse. De lo contrario, repetir desde el paso 2.\n",
    "\n",
    "Este algoritmo garantiza la convergencia a la política óptima en un número finito de iteraciones en ambientes con estados y acciones finitas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Policy evaluation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Input:} \\quad & \\pi,\\ \\text{la política a evaluar.} \\\\\n",
    "\\textbf{Parámetro:} \\quad & \\theta > 0,\\ \\text{umbral pequeño que determina la precisión.} \\\\\n",
    "\\textbf{Inicializar:} \\quad & V(s) \\text{ arbitrariamente para todo } s \\in S^+, \\text{ excepto } V(\\text{terminal}) = 0. \\\\\n",
    "\\\\\n",
    "\\textbf{Repetir:} \\quad & \\Delta \\gets 0 \\\\\n",
    "& \\text{Para cada } s \\in S: \\\\\n",
    "& \\quad\\quad v \\gets V(s) \\\\\n",
    "& \\quad\\quad V(s) \\gets \\sum_{a} \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)\\,\\Bigl[r + \\gamma V(s')\\Bigr] \\\\\n",
    "& \\quad\\quad \\Delta \\gets \\max\\Bigl(\\Delta,\\, \\bigl|v - V(s)\\bigr|\\Bigr) \\\\\n",
    "\\\\\n",
    "\\textbf{Hasta que:} \\quad & \\Delta < \\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(p, pi, gamma=GAMMA, theta=1e-5, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Evalúa la política 'pi' en el entorno 'env' usando el método de evaluación iterativa.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        pi: politica a evaluar, diccionario que mapea (estado, acción) a su probabilidad en la política.\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        V: función de valor de la política 'pi', diccionario que mapea estado (x, y) a su valor.\n",
    "    \"\"\"\n",
    "    V = {}\n",
    "    q = {}\n",
    "\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            V[s] = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                s = (x,y)\n",
    "                v = V[s]\n",
    "                pol = [pi.get((s, a), 0) for a in range(4)]\n",
    "\n",
    "                for a in range(4):\n",
    "                    q[(s,a)] = sum([prob * (r + gamma * V[s_siguiente]) for prob, s_siguiente, r in p[(s, a)]])\n",
    "\n",
    "                V[s] = sum([pol[a] * q[(s,a)] for a in range(4)])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pi_abajo.get(((0,1), a), 0) for a in range(4)]\n",
    "\n",
    "#print(pi_abajo[(0,1),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -5.85    -4.72    -3.59\n",
      "   -5.44    -4.93    -3.71    -2.48\n",
      "   -4.98    -3.78    -2.51    -1.25\n",
      "   -4.12    -2.77    -1.39     0.00\n"
     ]
    }
   ],
   "source": [
    "V_da_pi = policy_evaluation(p, pi_der_aba)\n",
    "print_state_values(V_da_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00  -100.00  -100.00    -3.54\n",
      "  -81.58  -100.00  -100.00    -2.46\n",
      "  -90.44  -100.00  -100.00    -1.24\n",
      "  -91.32  -100.00  -100.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_d_pi = policy_evaluation(p, pi_abajo, gamma=0.99)\n",
    "print_state_values(V_d_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00   -12.97   -17.59   -18.61\n",
      "  -11.23   -15.60   -17.22   -16.55\n",
      "  -16.55   -17.22   -15.60   -11.23\n",
      "  -18.61   -17.59   -12.97     0.00\n"
     ]
    }
   ],
   "source": [
    "V_pi_random = policy_evaluation(p, pi_rand, gamma=0.99)\n",
    "print_state_values(V_pi_random, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Policy Improvement\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Input:} \\quad & V,\\ \\text{la función de valor evaluada}, \\\\\n",
    "& p,\\ \\text{la dinamica del ambiente } \\\\\n",
    "& \\gamma,\\ \\text{el factor de descuento.} \\\\\n",
    "\\\\\n",
    "\\textbf{Inicializar:} \\quad & \\text{Para cada } s \\in S \\text{ y } a \\in A(s),\\ new\\_pi(s,a) \\text{ se asigna arbitrariamente.} \\\\\n",
    "\\\\\n",
    "\\textbf{Para cada } s \\in S \\text{ (excepto estados terminales):} \\quad & \\\\\n",
    "& \\quad \\text{Para cada acción } a \\in A(s): \\\\\n",
    "& \\quad\\quad Q(s,a) \\gets \\sum_{s',r} p(s',r \\mid s,a) \\Bigl[ r + \\gamma\\, V(s') \\Bigr] \\\\\n",
    "& \\quad \\text{Definir } best\\_actions = \\{ a \\in A(s) \\mid Q(s,a) = \\max_{a' \\in A(s)} Q(s,a') \\} \\\\\n",
    "& \\quad \\text{Para cada } a \\in A(s): \\\\\n",
    "& \\quad\\quad new\\_pi(s,a) \\gets \n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{1}{\\lvert best\\_actions \\rvert}, & \\text{si } a \\in best\\_actions, \\\\\n",
    "0, & \\text{si } a \\notin best\\_actions.\n",
    "\\end{cases} \\\\\n",
    "\\\\\n",
    "\\textbf{Retornar:} \\quad & new\\_pi.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Q(p, V, gamma=GAMMA, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Calcula el valor Q(s, a) para cada estado y acción.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        V: función de valor, diccionario que mapea estado (x, y) a su valor.\n",
    "        gamma: factor de descuento.\n",
    "    \n",
    "    Retorna:\n",
    "        Q: valor Q(s, a).\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x, y)\n",
    "            for a in range(4):\n",
    "                Q[(s, a)] = sum([prob * (r + gamma * V[next_s]) for (prob,next_s,r) in p[(s, a)]])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00   -17.93   -19.37   -19.41\n",
      "  -15.14   -17.80   -17.56   -17.22\n",
      "  -17.76   -16.80   -13.00   -11.64\n",
      "  -18.59   -14.86    -3.86     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00   -14.23   -18.56   -19.41\n",
      "   -2.65   -14.39   -18.39   -18.87\n",
      "  -12.97   -16.80   -17.80   -15.89\n",
      "  -17.76   -18.26   -16.34     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -3.86   -14.86   -18.59\n",
      "  -11.64   -13.00   -16.80   -17.76\n",
      "  -17.22   -17.56   -17.80   -15.14\n",
      "  -19.41   -19.37   -17.93     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00   -16.34   -18.26   -17.76\n",
      "  -15.89   -17.80   -16.80   -12.97\n",
      "  -18.87   -18.39   -14.39    -2.65\n",
      "  -19.41   -18.56   -14.23     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_pi_random = calculate_Q(p, V_pi_random)\n",
    "print_state_action_values(Q_pi_random, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00    -5.85    -4.72    -4.48\n",
      "   -5.44    -4.93    -3.71    -3.47\n",
      "   -4.98    -3.78    -2.51    -2.25\n",
      "   -4.12    -2.77    -1.39     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00    -6.76    -5.62    -4.48\n",
      "   -1.50    -6.64    -5.50    -4.36\n",
      "   -6.31    -5.71    -4.48    -3.23\n",
      "   -5.89    -4.68    -3.40     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -2.08    -6.52    -5.38\n",
      "   -5.85    -6.32    -5.67    -4.45\n",
      "   -5.94    -5.75    -4.53    -3.26\n",
      "   -5.21    -4.95    -3.60     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00    -6.02    -4.81    -3.59\n",
      "   -5.48    -4.98    -3.73    -2.48\n",
      "   -5.25    -3.98    -2.62    -1.25\n",
      "   -5.21    -3.87    -2.50     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_da_pi = calculate_Q(p, V_da_pi)\n",
    "print_state_action_values(Q_da_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00  -100.00   -23.60    -4.40\n",
      "  -89.15  -100.00   -22.75    -3.42\n",
      "  -97.32  -100.00   -21.78    -2.23\n",
      "  -98.19  -100.00   -20.80     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00  -100.00  -100.00    -4.40\n",
      "   -9.95  -100.00  -100.00    -4.28\n",
      "  -82.73  -100.00  -100.00    -3.19\n",
      "  -90.62  -100.00  -100.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00   -20.80  -100.00   -80.79\n",
      "  -74.57   -85.41  -100.00   -80.67\n",
      "  -89.75   -92.43  -100.00   -80.44\n",
      "  -91.32   -93.12  -100.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00  -100.00  -100.00    -3.54\n",
      "  -81.58  -100.00  -100.00    -2.46\n",
      "  -90.44  -100.00  -100.00    -1.24\n",
      "  -91.32  -100.00  -100.00     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_d_pi = calculate_Q(p, V_d_pi, gamma=0.99)\n",
    "print_state_action_values(Q_d_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Policy Improvement\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Input:} \\quad & V,\\ \\text{la función de valor evaluada}, \\\\\n",
    "& p,\\ \\text{la dinamica del ambiente } \\\\\n",
    "& \\gamma,\\ \\text{el factor de descuento.} \\\\\n",
    "\\\\\n",
    "\\textbf{Inicializar:} \\quad & \\text{Para cada } s \\in S \\text{ y } a \\in A(s),\\ new\\_pi(s,a) \\text{ se asigna arbitrariamente.} \\\\\n",
    "\\\\\n",
    "\\textbf{Para cada } s \\in S \\text{ (excepto estados terminales):} \\quad & \\\\\n",
    "& \\quad \\text{Para cada acción } a \\in A(s): \\\\\n",
    "& \\quad\\quad Q(s,a) \\gets \\sum_{s',r} p(s',r \\mid s,a) \\Bigl[ r + \\gamma\\, V(s') \\Bigr] \\\\\n",
    "& \\quad \\text{Definir } best\\_actions = \\{ a \\in A(s) \\mid Q(s,a) = \\max_{a' \\in A(s)} Q(s,a') \\} \\\\\n",
    "& \\quad \\text{Para cada } a \\in A(s): \\\\\n",
    "& \\quad\\quad new\\_pi(s,a) \\gets \n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{1}{\\lvert best\\_actions \\rvert}, & \\text{si } a \\in best\\_actions, \\\\\n",
    "0, & \\text{si } a \\notin best\\_actions.\n",
    "\\end{cases} \\\\\n",
    "\\\\\n",
    "\\textbf{Retornar:} \\quad & new\\_pi.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(p, V, gamma=GAMMA, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Genera una política mejorada (greedy) a partir de la función de valor V.\n",
    "    \n",
    "    Parámetros:\n",
    "        env: entorno que posee un atributo 'p' con la dinámica.\n",
    "        V: diccionario que mapea estados a sus valores.\n",
    "        gamma: factor de descuento.\n",
    "    \n",
    "    Retorna:\n",
    "        new_pi: diccionario que representa la política mejorada, mapeando (estado, acción) a probabilidad.\n",
    "    \"\"\"\n",
    "\n",
    "    new_pi = {}\n",
    "    Q = calculate_Q(p, V, gamma)\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x, y)\n",
    "            if is_done(s):\n",
    "                continue\n",
    "            lista = np.array([Q[(s, a)] for a in range(4)])\n",
    "            lista_indices = np.where(lista == np.max(lista))[0]\n",
    "            for a in range(4):\n",
    "                #new_pi[(s, a)] = 1 if a == mejor_a else 0\n",
    "                new_pi[(s, a)] = 1/len(lista_indices) if a in lista_indices else 0\n",
    "    \n",
    "    return new_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_policies(pi_1, pi_2, tol=1e-6, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Compara dos políticas y retorna True si son iguales (considerando una tolerancia en los valores), o False en caso contrario.\n",
    "    \n",
    "    Parámetros:\n",
    "        pi_1, pi_2: diccionarios que representan las políticas (mapean (estado, acción) a probabilidad).\n",
    "        tol: tolerancia para comparar las probabilidades.\n",
    "    \n",
    "    Retorna:\n",
    "        True si ambas políticas son iguales en todos los (estado, acción); False de lo contrario.\n",
    "    \"\"\"\n",
    "    delta = 0\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x, y)\n",
    "            for a in range(4):\n",
    "                if is_done(s):\n",
    "                    continue\n",
    "                # Comparamos las probabilidades de ambas políticas para el estado s y la acción a\n",
    "                delta = max(delta, abs(pi_1.get((s, a), 0) - pi_2.get((s, a), 0)))\n",
    "\n",
    "    \n",
    "\n",
    "    return delta < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(p, pi_init, gamma=GAMMA, theta=1e-5, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Realiza el algoritmo de iteración de política.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        pi: política óptima.\n",
    "    \"\"\"\n",
    "    pi = pi_init\n",
    "    iteration_counter = 0\n",
    "\n",
    "    while True:\n",
    "        V = policy_evaluation(p, pi, gamma, theta, size)\n",
    "        new_pi = improve_policy(p, V, gamma=GAMMA, size=GRID_SIZE)\n",
    "        if compare_policies(pi, new_pi):\n",
    "            break\n",
    "        else:\n",
    "            pi = new_pi\n",
    "            iteration_counter += 1\n",
    "        \n",
    "        \n",
    "\n",
    "    return pi, iteration_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star = policy_iteration(p, pi_rand)[0]\n",
    "print_policy_grids(pi_star, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        state = (obs[\"pos\"][0], obs[\"pos\"][1])\n",
    "        action = select_action(state, pi_star)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration es un algoritmo de programación dinámica empleado para estimar la función de valor óptima $ V^* $ y, a partir de ella, derivar la política óptima $ \\pi^* $. Este método se basa en actualizar iterativamente los valores de cada estado utilizando la ecuación de Bellman, hasta que las actualizaciones sean menores que un pequeño umbral $ \\theta $ que determina la precisión de la estimación.\n",
    "\n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Inicialización:**  \n",
    "   Se asigna un valor arbitrario a $ V(s) $ para todos los estados $ s \\in S^+ $, excepto en los estados terminales, donde se establece $ V(\\text{terminal}) = 0 $.\n",
    "\n",
    "2. **Actualización Iterativa:**  \n",
    "   Para cada estado $ s \\in S $, se actualiza su valor mediante la fórmula:\n",
    "   $$\n",
    "   V(s) \\leftarrow \\max_{a} \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma V(s') \\Bigr],\n",
    "   $$\n",
    "   donde $ p(s', r \\mid s, a) $ es la probabilidad de transitar al estado $ s' $ y recibir la recompensa $ r $ al tomar la acción $ a $ en el estado $ s $.  \n",
    "   La iteración continúa hasta que la diferencia máxima entre los valores antiguos y actualizados en todos los estados sea menor que $ \\theta $.\n",
    "\n",
    "3. **Extracción de la Política:**  \n",
    "   Una vez convergido $ V $, se define la política óptima determinista $ \\pi^* $ de la siguiente manera:\n",
    "   $$\n",
    "   \\pi^*(s) = \\operatorname*{argmax}_{a} \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma V(s') \\Bigr].\n",
    "   $$\n",
    "\n",
    "Este algoritmo es fundamental en el aprendizaje por refuerzo y en la resolución de procesos de decisión de Markov, ya que permite obtener de manera eficiente tanto la función de valor óptima como la política que maximiza el retorno esperado en cada estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(p, gamma=GAMMA, theta=1e-5, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Realiza el algoritmo de iteración de valor.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        pi: la pólitica óptima.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Inicializamos la función de valor V y la política pi\n",
    "    V = {(x,y):0 for x in range(size) for y in range(size)}\n",
    "    V[(0,0)] = 0\n",
    "    V[(size-1,size-1)] = 0\n",
    "    V_prev = V.copy()\n",
    "    dict_actions = {(x,y):[] for x in range(size) for y in range(size)}\n",
    "    pi = {}\n",
    "\n",
    "\n",
    "    while True:\n",
    "        #print(\"entro\")\n",
    "\n",
    "        q = calculate_Q(p, V_prev, gamma, size)\n",
    "        \n",
    "        for x in range(size):\n",
    "            for y in range(size):\n",
    "                s = (x, y)\n",
    "                if is_done(s):\n",
    "                    continue\n",
    "                \n",
    "                #print(calculate_Q(p, V_prev, gamma, size))\n",
    "\n",
    "                \n",
    "                q_array = np.array([q[s,a] for a in range(4)])\n",
    "                max_indexes = np.where(q_array == np.max(q_array))[0]\n",
    "\n",
    "                #if np.max(q_array) >= V[s]:\n",
    "\n",
    "                dict_actions[s] = max_indexes.tolist()\n",
    "                V[s] = np.max(q_array)\n",
    "        \n",
    "\n",
    "        # Comprobamos la convergencia\n",
    "\n",
    "        delta = max(abs(V[s] - V_prev[s]) for s in V.keys())\n",
    "        if delta < theta:\n",
    "            break\n",
    "        else:\n",
    "            V_prev = V.copy()   \n",
    "\n",
    "    for state,actions in dict_actions.items():\n",
    "        for action in actions:\n",
    "                pi[(state, action)] = 1 / len(actions)\n",
    "\n",
    "    return pi\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star2 = value_iteration(p)\n",
    "print_policy_grids(pi_star2, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        state = (obs[\"pos\"][0], obs[\"pos\"][1])\n",
    "        action = select_action(state, pi_star2)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos la dinámica del ambiente estocástico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 0): [(1.0, (0, 0), 0.0)],\n",
       " ((0, 0), 1): [(1.0, (0, 0), 0.0)],\n",
       " ((0, 0), 2): [(1.0, (0, 0), 0.0)],\n",
       " ((0, 0), 3): [(1.0, (0, 0), 0.0)],\n",
       " ((0, 1), 0): [(0.8, (1, 1), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)],\n",
       " ((0, 1), 1): [(0.8, (0, 0), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)],\n",
       " ((0, 1), 2): [(0.8, (0, 1), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)],\n",
       " ((0, 1), 3): [(0.8, (0, 2), -1.0), (0.1, (0, 0), -1.0), (0.1, (0, 2), -1.0)],\n",
       " ((0, 2), 0): [(0.8, (1, 2), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 2), 1): [(0.8, (0, 1), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 2), 2): [(0.8, (0, 2), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 2), 3): [(0.8, (0, 3), -1.0), (0.1, (0, 1), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 3), 0): [(0.8, (1, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 3), 1): [(0.8, (0, 2), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 3), 2): [(0.8, (0, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((0, 3), 3): [(0.8, (0, 3), -1.0), (0.1, (0, 2), -1.0), (0.1, (0, 3), -1.0)],\n",
       " ((1, 0), 0): [(0.8, (2, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)],\n",
       " ((1, 0), 1): [(0.8, (1, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)],\n",
       " ((1, 0), 2): [(0.8, (0, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)],\n",
       " ((1, 0), 3): [(0.8, (1, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 1), -1.0)],\n",
       " ((1, 1), 0): [(0.8, (2, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)],\n",
       " ((1, 1), 1): [(0.8, (1, 0), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)],\n",
       " ((1, 1), 2): [(0.8, (0, 1), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)],\n",
       " ((1, 1), 3): [(0.8, (1, 2), -1.0), (0.1, (1, 0), -1.0), (0.1, (1, 2), -1.0)],\n",
       " ((1, 2), 0): [(0.8, (2, 2), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 2), 1): [(0.8, (1, 1), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 2), 2): [(0.8, (0, 2), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 2), 3): [(0.8, (1, 3), -1.0), (0.1, (1, 1), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 3), 0): [(0.8, (2, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 3), 1): [(0.8, (1, 2), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 3), 2): [(0.8, (0, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((1, 3), 3): [(0.8, (1, 3), -1.0), (0.1, (1, 2), -1.0), (0.1, (1, 3), -1.0)],\n",
       " ((2, 0), 0): [(0.8, (3, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)],\n",
       " ((2, 0), 1): [(0.8, (2, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)],\n",
       " ((2, 0), 2): [(0.8, (1, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)],\n",
       " ((2, 0), 3): [(0.8, (2, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 1), -1.0)],\n",
       " ((2, 1), 0): [(0.8, (3, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)],\n",
       " ((2, 1), 1): [(0.8, (2, 0), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)],\n",
       " ((2, 1), 2): [(0.8, (1, 1), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)],\n",
       " ((2, 1), 3): [(0.8, (2, 2), -1.0), (0.1, (2, 0), -1.0), (0.1, (2, 2), -1.0)],\n",
       " ((2, 2), 0): [(0.8, (3, 2), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 2), 1): [(0.8, (2, 1), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 2), 2): [(0.8, (1, 2), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 2), 3): [(0.8, (2, 3), -1.0), (0.1, (2, 1), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 3), 0): [(0.8, (3, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 3), 1): [(0.8, (2, 2), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 3), 2): [(0.8, (1, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((2, 3), 3): [(0.8, (2, 3), -1.0), (0.1, (2, 2), -1.0), (0.1, (2, 3), -1.0)],\n",
       " ((3, 0), 0): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)],\n",
       " ((3, 0), 1): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)],\n",
       " ((3, 0), 2): [(0.8, (2, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)],\n",
       " ((3, 0), 3): [(0.8, (3, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 1), -1.0)],\n",
       " ((3, 1), 0): [(0.8, (3, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)],\n",
       " ((3, 1), 1): [(0.8, (3, 0), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)],\n",
       " ((3, 1), 2): [(0.8, (2, 1), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)],\n",
       " ((3, 1), 3): [(0.8, (3, 2), -1.0), (0.1, (3, 0), -1.0), (0.1, (3, 2), -1.0)],\n",
       " ((3, 2), 0): [(0.8, (3, 2), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)],\n",
       " ((3, 2), 1): [(0.8, (3, 1), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)],\n",
       " ((3, 2), 2): [(0.8, (2, 2), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)],\n",
       " ((3, 2), 3): [(0.8, (3, 3), -1.0), (0.1, (3, 1), -1.0), (0.1, (3, 3), -1.0)],\n",
       " ((3, 3), 0): [(1.0, (3, 3), 0.0)],\n",
       " ((3, 3), 1): [(1.0, (3, 3), 0.0)],\n",
       " ((3, 3), 2): [(1.0, (3, 3), 0.0)],\n",
       " ((3, 3), 3): [(1.0, (3, 3), 0.0)]}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00  -100.00  -100.00    -3.54\n",
      "  -81.58  -100.00  -100.00    -2.46\n",
      "  -90.44  -100.00  -100.00    -1.24\n",
      "  -91.32  -100.00  -100.00     0.00\n",
      "\n",
      "\n",
      "\n",
      "Cantidad de Iteraciones: 2 \n",
      "\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_state_values(V_d_pi, GRID_SIZE)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "pol_abajo,iterations_abajo = policy_iteration(p, pi_abajo, 0.9, theta=1e-5, size=GRID_SIZE)\n",
    "\n",
    "print(f\"Cantidad de Iteraciones: {iterations_abajo} \\n\")\n",
    "print_policy_grids(pol_abajo, GRID_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -5.85    -4.72    -3.59\n",
      "   -5.44    -4.93    -3.71    -2.48\n",
      "   -4.98    -3.78    -2.51    -1.25\n",
      "   -4.12    -2.77    -1.39     0.00\n",
      "\n",
      "\n",
      "\n",
      "Cantidad de Iteraciones: 4 \n",
      "\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_state_values(V_da_pi, GRID_SIZE)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "pol_der_aba,iterations_der_aba = policy_iteration(p, pi_der_aba, 0.9, theta=1e-5, size=GRID_SIZE)\n",
    "\n",
    "print(f\"Cantidad de Iteraciones: {iterations_der_aba} \\n\")\n",
    "print_policy_grids(pol_der_aba, GRID_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00   -12.97   -17.59   -18.61\n",
      "  -11.23   -15.60   -17.22   -16.55\n",
      "  -16.55   -17.22   -15.60   -11.23\n",
      "  -18.61   -17.59   -12.97     0.00\n",
      "\n",
      "\n",
      "\n",
      "Cantidad de Iteraciones: 2 \n",
      "\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   0.00   1.00   0.00\n",
      "  0.00   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "  1.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   1.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_state_values(V_pi_random, GRID_SIZE)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "pol_rand,iterations_rand = policy_iteration(p, pi_rand, 0.9, theta=1e-5, size=GRID_SIZE)\n",
    "\n",
    "print(f\"Cantidad de Iteraciones: {iterations_rand} \\n\")\n",
    "print_policy_grids(pol_rand, GRID_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. [extra] Evaluación en un entorno estocástico**\n",
    "- ¿Cómo cambia la política óptima en este caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso podemos observar, en primer lugar, que la política óptima, a diferencia con el caso anterior, es deterministica.\n",
    "Esto ocurre debido a que por cómo esta definido el ambiente estocastico, al realizar cualquier acción, el agente tiene un 10% de probabilidad de ir hacia arriba, un 10% de ir hacia abajo y un 80% de ir hacia la dirección \"deseada\". Por lo tanto a diferencia que en el ambiente determinístico, los estados con mejores valores luego de los terminales son el (1,0) y el (2,3) ya que la aleatoriedad les otorga un 10% de probabilidades extra de llegar al estado terminal sobre los otros estados que están pegados al terminal como lo son el (0,1) y el (3,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
